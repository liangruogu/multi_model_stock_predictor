"""
Data Preprocessor Module

Handles loading, cleaning, and preprocessing of stock market data for LSTM model training.
"""

import pandas as pd
import numpy as np
import sys
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import Dataset, DataLoader
from typing import Tuple, Optional
from loguru import logger
from copy import deepcopy as dc

# é…ç½®æ—¥å¿— - æš‚æ—¶å…³é—­æ‰€æœ‰æ—¥å¿—
logger.remove()
# logger.add(
#     sys.stderr,
#     format="<level>{message}</level>",
#     level="INFO"
# )


class StockDataset(Dataset):
    """PyTorch Dataset for stock price sequences."""

    def __init__(self, data: np.ndarray, sequence_length: int = 30):
        """
        Initialize the dataset.

        Args:
            data: Preprocessed stock data
            sequence_length: Length of input sequences
        """
        self.data = data
        self.sequence_length = sequence_length

    def __len__(self) -> int:
        return len(self.data) - self.sequence_length

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Get a sample from the dataset.

        Args:
            idx: Index of the sample

        Returns:
            Tuple of input sequence and target value
        """
        x = self.data[idx:idx + self.sequence_length]
        y = self.data[idx + self.sequence_length]

        # è½¬æ¢ä¸ºå¼ é‡
        x_tensor = torch.FloatTensor(x)
        y_tensor = torch.FloatTensor([y])

        # è®°å½•å…³é”®æ ·æœ¬ä¿¡æ¯ï¼ˆåªåœ¨ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰
        if idx == 0:
            logger.info(f"ğŸ“Š æ ·æœ¬0: è¾“å…¥åºåˆ—ä»·æ ¼èŒƒå›´ ${x_tensor.min():.2f}-${x_tensor.max():.2f}, ç›®æ ‡ä»·æ ¼ ${y_tensor.item():.2f}")

        return x_tensor, y_tensor


class BlogStyleDataPreprocessor:
    """åšå®¢é£æ ¼çš„æ•°æ®é¢„å¤„ç†å™¨ - ä½¿ç”¨æ»åç‰¹å¾å’ŒMinMaxScaler"""

    def __init__(self, n_steps: int = 60):
        """
        Initialize the blog-style data preprocessor.

        Args:
            n_steps: Number of lookback steps (like blog's lookback)
        """
        self.n_steps = n_steps
        self.scaler = MinMaxScaler(feature_range=(-1, 1))  # åšå®¢ä½¿ç”¨(-1,1)
        self.data: Optional[pd.DataFrame] = None

    def prepare_dataframe_for_lstm(self, df: pd.DataFrame, n_steps: int) -> pd.DataFrame:
        """
        å¤„ç†æ•°æ®é›†ï¼Œä½¿å…¶é€‚ç”¨äºLSTMæ¨¡å‹ - åšå®¢æ–¹æ³•
        """
        df = dc(df)

        # ç¡®ä¿æœ‰dateåˆ—
        if 'trade_date' in df.columns:
            df['date'] = pd.to_datetime(df['trade_date'])
        elif 'date' not in df.columns:
            df['date'] = pd.to_datetime(df.index)

        df.set_index('date', inplace=True)

        # åˆ›å»ºæ»åç‰¹å¾
        for i in range(1, n_steps+1):
            df[f'close(t-{i})'] = df['close'].shift(i)

        df.dropna(inplace=True)
        return df

    def get_dataset(self, file_path: str, lookback: int = None, split_ratio: float = 0.9):
        """
        å½’ä¸€åŒ–æ•°æ®ã€åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† - åšå®¢æ–¹æ³•
        """
        if lookback is None:
            lookback = self.n_steps

        data = pd.read_csv(file_path)

        # åªä½¿ç”¨dateå’Œcloseåˆ—
        if 'trade_date' in data.columns:
            data = data[['trade_date','close']]
            data.rename(columns={'trade_date': 'date'}, inplace=True)
        else:
            data = data[['date','close']]

        shifted_df = self.prepare_dataframe_for_lstm(data, lookback)

        # ä½¿ç”¨MinMaxScalerå½’ä¸€åŒ–åˆ°(-1,1)
        shifted_np = self.scaler.fit_transform(shifted_df)

        X = shifted_np[:, 1:]  # æ»åç‰¹å¾
        y = shifted_np[:, 0]   # å½“å‰å€¼

        # åè½¬æ—¶é—´é¡ºåºï¼Œåƒåšå®¢ä¸€æ ·
        X = dc(np.flip(X, axis=1))

        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        split_index = int(len(X) * split_ratio)

        X_train = X[:split_index]
        X_test = X[split_index:]

        y_train = y[:split_index]
        y_test = y[split_index:]

        # é‡å¡‘ä¸ºLSTMè¾“å…¥æ ¼å¼
        X_train = X_train.reshape((-1, lookback, 1))
        X_test = X_test.reshape((-1, lookback, 1))

        y_train = y_train.reshape((-1, 1))
        y_test = y_test.reshape((-1, 1))

        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        X_train = torch.tensor(X_train).float()
        y_train = torch.tensor(y_train).float()
        X_test = torch.tensor(X_test).float()
        y_test = torch.tensor(y_test).float()

        return self.scaler, X_train, X_test, y_train, y_test

    def inverse_transform(self, values: np.ndarray) -> np.ndarray:
        """
        åå½’ä¸€åŒ– - å°†å½’ä¸€åŒ–å€¼è½¬æ¢å›åŸå§‹ä»·æ ¼
        """
        # åˆ›å»ºä¸€ä¸ªå®Œæ•´å½¢çŠ¶çš„æ•°ç»„è¿›è¡Œåå½’ä¸€åŒ–
        dummies = np.zeros((len(values), self.n_steps + 1))
        dummies[:, 0] = values.flatten()

        # åå½’ä¸€åŒ–
        inversed = self.scaler.inverse_transform(dummies)
        return inversed[:, 0].reshape(-1, 1)

    def get_latest_sequence(self) -> np.ndarray:
        """
        è·å–æœ€æ–°çš„åºåˆ—ç”¨äºé¢„æµ‹ - åšå®¢é£æ ¼
        """
        # é‡æ–°è¯»å–æœ€æ–°æ•°æ®
        data = pd.read_csv('data/000001_daily_qfq_8y.csv')
        if 'trade_date' in data.columns:
            data = data[['trade_date','close']]
            data.rename(columns={'trade_date': 'date'}, inplace=True)
        else:
            data = data[['date','close']]

        # å‡†å¤‡æ•°æ®
        shifted_df = self.prepare_dataframe_for_lstm(data, self.n_steps)
        shifted_np = self.scaler.transform(shifted_df)

        # è·å–æœ€æ–°çš„åºåˆ—ï¼ˆåè½¬æ—¶é—´é¡ºåºï¼‰
        X = shifted_np[:, 1:]
        X = np.flip(X, axis=1)

        # è¿”å›æœ€åä¸€ä¸ªåºåˆ—
        return X[-1:].reshape(self.n_steps, 1)


class DataPreprocessor:
    """åŸå§‹æ•°æ®é¢„å¤„ç†å™¨ - ä¿ç•™å…¼å®¹æ€§"""

    def __init__(self, sequence_length: int = 30):
        """
        Initialize the data preprocessor.

        Args:
            sequence_length: Length of input sequences for LSTM
        """
        self.sequence_length = sequence_length
        self.scaler = MinMaxScaler(feature_range=(0, 1))
        self.data: Optional[pd.DataFrame] = None
        self.train_data: Optional[np.ndarray] = None
        self.test_data: Optional[np.ndarray] = None

    def load_data(self, file_path: str) -> pd.DataFrame:
        """
        Load stock data from CSV file.

        Args:
            file_path: Path to the CSV file containing stock data

        Returns:
            Loaded and preprocessed DataFrame
        """
        self.data = pd.read_csv(file_path)

        # Convert date format and set as index
        self.data['trade_date'] = pd.to_datetime(self.data['trade_date'])
        self.data.set_index('trade_date', inplace=True)

        # Sort by date
        self.data = self.data.sort_index()

        # Select feature columns
        features = ['open', 'close', 'high', 'low', 'volume']
        self.data = self.data[features]

        # Handle missing values
        self.data = self.data.ffill().bfill()

        print(f"Data loaded successfully: {len(self.data)} records")
        print(f"Date range: {self.data.index[0]} to {self.data.index[-1]}")

        return self.data

    def preprocess_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        æ•°æ®é¢„å¤„ç† - ä½¿ç”¨å½’ä¸€åŒ–å¤„ç†ï¼Œå¢å¼ºä»·æ ¼å˜åŒ–çš„æ•æ„Ÿæ€§

        Returns:
            Tuple of (train_data, test_data)
        """
        if self.data is None:
            raise ValueError("Data not loaded. Call load_data() first.")

        # ä½¿ç”¨æ”¶ç›˜ä»·
        close_prices = self.data['close'].values

        # å­˜å‚¨åŸå§‹ä»·æ ¼ä¿¡æ¯ï¼Œç”¨äºåå½’ä¸€åŒ–
        self.original_prices = close_prices
        self.price_mean = np.mean(close_prices)  # ä»·æ ¼å‡å€¼
        self.price_std = np.std(close_prices)    # ä»·æ ¼æ ‡å‡†å·®
        self.min_price = np.min(close_prices)    # æœ€å°ä»·æ ¼
        self.max_price = np.max(close_prices)    # æœ€å¤§ä»·æ ¼

        # å¤„ç†å¯èƒ½çš„å¼‚å¸¸å€¼
        close_prices = np.nan_to_num(close_prices, nan=0.0, posinf=0.0, neginf=0.0)

        print(f"ğŸ“Š åŸå§‹ä»·æ ¼ç»Ÿè®¡:")
        print(f"ä»·æ ¼èŒƒå›´: ${self.min_price:.2f} - ${self.max_price:.2f}")
        print(f"å¹³å‡ä»·æ ¼: ${self.price_mean:.2f}")
        print(f"ä»·æ ¼æ ‡å‡†å·®: ${self.price_std:.2f}")
        print(f"æ•°æ®ç‚¹æ•°é‡: {len(close_prices)}")

        # å…³é”®æ”¹è¿›: ä½¿ç”¨Z-scoreå½’ä¸€åŒ–ï¼Œå¢å¼ºå°å˜åŒ–çš„æ•æ„Ÿæ€§
        normalized_prices = (close_prices - self.price_mean) / self.price_std

        print(f"\nğŸ“ˆ å½’ä¸€åŒ–åç»Ÿè®¡:")
        print(f"å½’ä¸€åŒ–èŒƒå›´: [{normalized_prices.min():.3f}, {normalized_prices.max():.3f}]")
        print(f"å½’ä¸€åŒ–å‡å€¼: {normalized_prices.mean():.6f} (åº”è¯¥æ¥è¿‘0)")
        print(f"å½’ä¸€åŒ–æ ‡å‡†å·®: {normalized_prices.std():.6f} (åº”è¯¥æ¥è¿‘1)")

        # è½¬æ¢ä¸ºäºŒç»´æ•°ç»„æ ¼å¼
        price_data = normalized_prices.reshape(-1, 1)

        # Split training and testing sets
        train_size = len(price_data) - 120  # Last 120 days for testing

        train_data = price_data[:train_size]
        test_data = price_data[train_size - self.sequence_length:]

        self.train_data = train_data
        self.test_data = test_data

        print(f"\nè®­ç»ƒé›†å¤§å°: {len(train_data)}")
        print(f"æµ‹è¯•é›†å¤§å°: {len(test_data)}")
        print(f"è®­ç»ƒé›†å½’ä¸€åŒ–èŒƒå›´: [{train_data.min():.3f}, {train_data.max():.3f}]")
        print(f"æµ‹è¯•é›†å½’ä¸€åŒ–èŒƒå›´: [{test_data.min():.3f}, {test_data.max():.3f}]")

        return train_data, test_data

    def create_datasets(self, batch_size: int = 32) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """
        Create PyTorch datasets and data loaders.

        Args:
            batch_size: Batch size for training

        Returns:
            Tuple of (train_loader, val_loader, test_loader)
        """
        if self.train_data is None or self.test_data is None:
            raise ValueError("Data not preprocessed. Call preprocess_data() first.")

        # Create training and validation sets
        train_data, val_data = train_test_split(
            self.train_data,
            test_size=0.2,
            shuffle=False
        )

        train_dataset = StockDataset(train_data, self.sequence_length)
        val_dataset = StockDataset(val_data, self.sequence_length)
        test_dataset = StockDataset(self.test_data, self.sequence_length)

        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True
        )
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False
        )
        test_loader = DataLoader(
            test_dataset,
            batch_size=1,
            shuffle=False
        )

        return train_loader, val_loader, test_loader

    def inverse_transform(self, values: np.ndarray) -> np.ndarray:
        """
        åå½’ä¸€åŒ–ï¼šå°†å½’ä¸€åŒ–çš„é¢„æµ‹å€¼è½¬æ¢å›åŸå§‹ä»·æ ¼

        Args:
            values: Normalized predicted values

        Returns:
            Original price values
        """
        # åå½’ä¸€åŒ–å…¬å¼: original = normalized * std + mean
        original_values = values * self.price_std + self.price_mean
        return original_values.reshape(-1, 1)

    def convert_returns_to_prices(self, predicted_prices: np.ndarray, start_price: float = None) -> np.ndarray:
        """
        ç›´æ¥è¿”å›é¢„æµ‹çš„ä»·æ ¼ï¼Œå› ä¸ºå·²ç»é¢„æµ‹äº†ç»å¯¹ä»·æ ¼

        Args:
            predicted_prices: Predicted absolute prices (already in dollars)
            start_price: Not used, kept for compatibility

        Returns:
            Predicted absolute prices (same as input)
        """
        return predicted_prices.flatten()

    def get_latest_sequence(self) -> np.ndarray:
        """
        Get the latest sequence of returns for prediction.

        Returns:
            Latest sequence of returns
        """
        if self.train_data is None:
            raise ValueError("Data not preprocessed. Call preprocess_data() first.")

        return self.train_data[-self.sequence_length:]


if __name__ == "__main__":
    # Test data preprocessing
    preprocessor = DataPreprocessor(sequence_length=30)

    # Load data
    data = preprocessor.load_data("../../data/000001_daily_qfq_8y.csv")

    # Preprocess data
    train_data, test_data = preprocessor.preprocess_data()

    # Create data loaders
    train_loader, val_loader, test_loader = preprocessor.create_datasets(batch_size=32)

    print(f"Training batches: {len(train_loader)}")
    print(f"Validation batches: {len(val_loader)}")
    print(f"Testing batches: {len(test_loader)}")